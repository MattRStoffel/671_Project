diff --git a/.gitignore b/.gitignore
deleted file mode 100644
index 5bd1579..0000000
--- a/.gitignore
+++ /dev/null
@@ -1,3 +0,0 @@
-__pycache__
-results.pkl
-preprocessed_data.pkl
diff --git a/__pycache__/data.cpython-312.pyc b/__pycache__/data.cpython-312.pyc
new file mode 100644
index 0000000..a67de88
Binary files /dev/null and b/__pycache__/data.cpython-312.pyc differ
diff --git a/data.py b/data.py
index 4cc6119..cbf2b04 100644
--- a/data.py
+++ b/data.py
@@ -1,4 +1,3 @@
-from torch import tensor
 from torch.utils.data import Dataset, random_split, DataLoader
 import pandas as pd
 import re
@@ -7,68 +6,72 @@ from nltk.corpus import stopwords
 from nltk.tokenize import word_tokenize
 from nltk.stem import PorterStemmer
 from transformers import BertTokenizer
-import pickle
-import os
-import util
-from util import get_cpu_info
-
+import numpy as np
 
 class MyDataset(Dataset):
-    def __init__(self):
-        preprocessed_file = "preprocessed_data.pkl"
-        if not os.path.exists(preprocessed_file):
-            preprocess_and_save("ExtractedTweets.csv", preprocessed_file)
-
-        with open(preprocessed_file, "rb") as f:
-            self.data, self.vocabSize, self.maxSeq = pickle.load(f)
-
+    def __init__(self, batchsize: int):
+        df = pd.read_csv("./ExtractedTweets.csv")
+        try:
+            nltk.data.find("corpora/stopwords.zip")
+        except LookupError:
+            nltk.download("stopwords", quiet=True)
+            nltk.data.find("corpora/stopwords.zip")
+
+        try:
+            nltk.data.find("tokenizers/punkt.zip")
+        except LookupError:
+            nltk.download("punkt", quiet=True)
+            nltk.data.find("tokenizers/punkt.zip")
+
+
+        # Initializing a BERT google-bert/bert-base-uncased style configuration
+        self.maxSeq = 0
+        self.tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
+        self.vocabSize = self.getVocabSize()
+        self.stop_words = stopwords.words("english")
+        self.stemmer = PorterStemmer()
+
+        self.X = df["Tweet"].apply(self.preprocess_text)
+        self.Y = df["Party"]
+
+    def preprocess_text(self, text: str):
+        # Remove URLs they dont add valuable information
+        tokenized_tweet = []
+
+        try:
+            text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
+            # Remove punctuation and other unwanted characters
+            text = re.sub(r"[^\w\s]", "", text)
+            text = text.lower()
+            tokens = word_tokenize(text)
+            # Remove stop words and apply stemming
+            # Stemmers remove morphological affixes from words, leaving only the word stem
+            tokens = [
+                self.stemmer.stem(word) for word in tokens if word not in self.stop_words
+            ]
+            # Join tokens back to string
+
+
+            text = " ".join(tokens)
+            encode = self.tokenizer.encode(text, padding="max_length")
+            self.maxSeq = max(self.maxSeq, len(encode) )
+
+        except Exception as e:
+            print(e)
+            print(text)
+            exit(0)
+        return self.tokenizer.encode(text, padding="max_length", max_length=self.maxSeq)
+    def getVocabSize(self):
+        vocabSize = self.tokenizer.vocab_size
+        return vocabSize
     def __len__(self):
-        return len(self.data["tweets"])
+        return len(self.X)
 
     def __getitem__(self, idx):
-        tweet = self.data["tweets"][idx]
-        label = self.data["labels"][idx]
-        return tweet, label
-
-
-def preprocess_and_save(csv_file, preprocessed_file):
-    df = pd.read_csv(csv_file)
-    try:
-        nltk.data.find("corpora/stopwords.zip")
-    except LookupError:
-        nltk.download("stopwords", quiet=True)
-        nltk.data.find("corpora/stopwords.zip")
-
-    try:
-        nltk.data.find("tokenizers/punkt.zip")
-    except LookupError:
-        nltk.download("punkt", quiet=True)
-        nltk.data.find("tokenizers/punkt.zip")
-
-    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
-    stop_words = stopwords.words("english")
-    stemmer = PorterStemmer()
-    maxSeq = 128
-
-    def preprocess_text(text):
-        text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)
-        text = re.sub(r"[^\w\s]", "", text)
-        text = text.lower()
-        tokens = word_tokenize(text)
-        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
-        text = " ".join(tokens)
-        return tokenizer.encode(text, padding="max_length", max_length=maxSeq)
-
-    tweets = df["Tweet"].apply(preprocess_text).tolist()
-    labels = df["Party"].apply(definingLabel).tolist()
-
-    data = ({"tweets": tweets, "labels": labels}, tokenizer.vocab_size, maxSeq)
-    with open(preprocessed_file, "wb") as f:
-        pickle.dump(data, f)
-
+        return self.X[idx], self.Y[idx]
 
 def get_data_loaders(batch_size: int):
-    dataset = MyDataset()
+    dataset = MyDataset(batch_size)
     train_size = int(0.8 * len(dataset))
     test_size = int(0.8 * (len(dataset) - train_size))
     validate_size = len(dataset) - train_size - test_size
@@ -80,28 +83,11 @@ def get_data_loaders(batch_size: int):
         test_dataset, [test_size, validate_size]
     )
 
-    _, num_workers = get_cpu_info()
-
-    vocabSize = dataset.vocabSize
-    train_loader = DataLoader(
-        train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True
-    )
-    test_loader = DataLoader(
-        test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True
-    )
-    validation_loader = DataLoader(
-        validation_dataset,
-        batch_size=batch_size,
-        num_workers=num_workers,
-        pin_memory=True,
-    )
+    vocabSize = dataset.getVocabSize()
 
+    train_loader = DataLoader(train_dataset, batch_size=batch_size)
+    test_loader = DataLoader(test_dataset, batch_size=batch_size)
+    validation_loader = DataLoader(validation_dataset, batch_size=batch_size)
     return dataset.maxSeq, vocabSize, train_loader, test_loader, validation_loader
 
 
-def definingLabel(label: str):
-    if label.lower() == "republican":
-        y = [0.0, 1.0]
-    else:
-        y = [1.0, 0.0]
-    return tensor(y).to(util.get_device())
diff --git a/model.py b/model.py
index 04e871a..4e69e8c 100644
--- a/model.py
+++ b/model.py
@@ -1,4 +1,13 @@
 import torch
+import torch.nn.functional as F
+from data import get_data_loaders
+
+device = (
+    "cuda"
+    if torch.cuda.is_available()
+    else "mps" if torch.backends.mps.is_available() else "cpu"
+)
+print(f"Using {device} device")
 
 
 class NeuralNetwork(torch.nn.Module):
@@ -9,7 +18,7 @@ class NeuralNetwork(torch.nn.Module):
         self.layer1 = torch.nn.Linear(150, 40)
         # output: (batchSize x maxSeq x 4)
         self.layer2 = torch.nn.Linear(40, 2)
-        self.layer3 = torch.nn.Linear(maxSeq, 1)
+        self.laye3 = torch.nn.Linear(maxSeq, 1)
 
         self.softmax = torch.nn.Softmax(dim=1)
 
@@ -19,6 +28,65 @@ class NeuralNetwork(torch.nn.Module):
         output = self.layer1(output)
         output = self.layer2(output)  # ( batchsize x maxSeq x 2)
         output = torch.transpose(output, 1, 2)
-        output = self.layer3(output)
+        output = self.laye3(output)
         output = self.softmax(output)
         return output
+
+
+def definingLabel(label: str):
+    if label.lower() == "republican":
+        y = [0.0, 1.0]
+    else:
+        y = [1.0, 0.0]
+    return torch.tensor(y)
+
+
+# Access maxSeq and vocabSize
+batchsize = 3
+maxSeq, vocabSize, train_loader, test_loader, validation_loader = get_data_loaders(batchsize)
+model = NeuralNetwork(batchSize=batchsize, maxSeq=maxSeq, vocabSize=vocabSize)
+optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
+device = 'cpu'
+
+#Train model
+epochs = 500
+for epoch in range(epochs):
+    correct_count = 0
+    incorrect_count = 0
+    model = model.train()
+    for X, y in train_loader:
+        X = torch.stack(X, dim=1).int().to(device)
+        listOfLabels = []
+        for label in y:
+            listOfLabels.append(definingLabel(label))
+        listOfLabels = torch.stack(listOfLabels, dim=0).int().to(device)
+        pred = model.forward(X)
+        pred = pred.squeeze(dim=2)
+        #****************DELETE (debug) *****************************************
+        predVal = pred.argmax(dim=1)
+        actVal = listOfLabels.argmax(dim=1)
+        isCorrect = (predVal == actVal)
+        isWrong = (predVal != actVal)
+        correct_count = correct_count + isCorrect.sum().item()
+        incorrect_count = incorrect_count + isWrong.sum().item()
+        # ****************DELETE (debug) *****************************************
+        loss = F.cross_entropy(pred.float(), listOfLabels.float())
+        optimizer.zero_grad()
+        loss.backward()
+        optimizer.step()
+    print("model correct: ", correct_count)
+    print("model incorrect: ", incorrect_count)
+
+#Testing
+#for X, y in test_loader:
+#    X = torch.stack(X, dim=1).int().to(device)
+#    listOfLabels = []
+#    for label in y:
+#       listOfLabels.append(definingLabel(label))
+#        listOfLabels = torch.stack(listOfLabels, dim=0).int().to(device)
+#        pred = model.forward(X)
+#        pred = pred.squeeze(dim=2)
+#        loss = F.cross_entropy(pred.float(), listOfLabels.float())
+#        optimizer.zero_grad()
+#        loss.backward()
+#        optimizer.step()
diff --git a/train.py b/train.py
deleted file mode 100644
index 6197229..0000000
--- a/train.py
+++ /dev/null
@@ -1,108 +0,0 @@
-import torch
-import torch.nn.functional as F
-from model import NeuralNetwork
-import data
-import util
-from util import get_cpu_info
-cpu_name, num_threads = get_cpu_info()
-
-device = util.get_device()
-
-
-def train_one_epoch(model, train_loader, optimizer, device):
-    model.train()
-    total_loss = 0
-    for X, y in train_loader:
-        X = torch.stack(X, dim=1).int().to(device)
-        listOfLabels = [label for label in y]
-        listOfLabels = torch.stack(listOfLabels, dim=0).int()
-        pred = model(X).squeeze(dim=2)
-        loss = F.cross_entropy(pred.float(), listOfLabels.float())
-        optimizer.zero_grad()
-        loss.backward()
-        optimizer.step()
-        total_loss += loss.item()
-    return total_loss / len(train_loader)
-
-
-def validate(model, validation_loader, device):
-    model.eval()
-    correct_count = 0
-    incorrect_count = 0
-    with torch.no_grad():
-        for X, y in validation_loader:
-            X = torch.stack(X, dim=1).int().to(device)
-            listOfLabels = [label for label in y]
-            listOfLabels = torch.stack(listOfLabels, dim=0).int()
-            pred = model(X).squeeze(dim=2)
-            predVal = pred.argmax(dim=1)
-            actVal = listOfLabels.argmax(dim=1)
-            correct_count += (predVal == actVal).sum().item()
-            incorrect_count += (predVal != actVal).sum().item()
-    return correct_count / (correct_count + incorrect_count)
-
-
-def train(epochs=5, batchsize=200, learning_rate=0.001):
-    maxSeq, vocabSize, train_loader, _, validation_loader = data.get_data_loaders(
-        batchsize
-    )
-    model = NeuralNetwork(batchSize=batchsize, maxSeq=maxSeq, vocabSize=vocabSize).to(
-        device
-    )
-    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
-
-    validation_accuracy = []
-    trainings_loss = []
-
-    for epoch in range(epochs):
-        average_loss = train_one_epoch(model, train_loader, optimizer, device)
-        trainings_loss.append(average_loss)
-        validation_acc = validate(model, validation_loader, device)
-        validation_accuracy.append(validation_acc)
-        print(
-            f"Epoch {epoch + 1}, Loss: {average_loss:.5f}, Validation Accuracy: {validation_acc:.5f}"
-        )
-
-    return model, trainings_loss, validation_accuracy
-
-
-def my_grid_search():
-    epochs = [4, 8, 16, 32]
-    batchsizes = [200, 500, 1000]
-    learning_rates = [0.001, 0.01, 0.1]
-    results = {}
-    for epoch in epochs:
-        for batchsize in batchsizes:
-            for learning_rate in learning_rates:
-                print(
-                    f"Training with epochs: {epoch}, batchsize: {batchsize}, learning_rate: {learning_rate}"
-                )
-                model, trainings_loss, validation_accuracy = train(
-                    epoch, batchsize, learning_rate
-                )
-                results[(epoch, batchsize, learning_rate)] = (
-                    model,
-                    trainings_loss,
-                    validation_accuracy,
-                )
-                util.save_results(results, "results.pkl")
-
-    # print top 5 models
-    results = sorted(results.items(), key=lambda x: x[1][2], reverse=True)
-    for i, (params, (_, _, validation_accuracy)) in enumerate(results):
-        print(f"Model {i+1}: {params}, Validation Accuracy: {validation_accuracy[-1]}")
-
-
-if __name__ == "__main__":
-    print(f"Using {device} device")
-    if device == "cuda":
-        print("Is CUDA available:", torch.cuda.is_available())
-        print("CUDA device count:", torch.cuda.device_count())
-        print("Current device:", torch.cuda.current_device())
-        print(
-            "Device name:",
-            torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None",
-        )
-        print(f"Detected CPU: {cpu_name}")
-        print(f"Setting number of CPU threads: {num_threads}")
-    my_grid_search()
diff --git a/util.py b/util.py
deleted file mode 100644
index b69f881..0000000
--- a/util.py
+++ /dev/null
@@ -1,35 +0,0 @@
-from torch import cuda, backends
-import pickle
-import matplotlib as plt
-import multiprocessing
-import cpuinfo # pip install py-cpuinfo
-
-def get_device():
-    device = (
-        "cuda"
-        if cuda.is_available()
-        else "mps"
-        if backends.mps.is_available()
-        else "cpu"
-    )
-    return device
-
-
-def get_cpu_info():
-    try:
-        cpu_info = cpuinfo.get_cpu_info()
-        cpu_name = cpu_info.get("brand_raw", "Unknown Processor")
-        cpu_threads = multiprocessing.cpu_count()
-
-        return cpu_name, cpu_threads
-    except Exception as e:
-        print(f"An error occurred while detecting CPU info: {e}")
-        return "Unknown Processor", 1  # in case of failure
-
-
-def graph_results():
-    plt.show()
-
-
-def generate_latex():
-    pass
